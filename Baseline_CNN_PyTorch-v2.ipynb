{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto update from code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "\n",
    "from src.model_api_pytorch import ModifiedCNNv2, train_model\n",
    "from src.data import ProductDataset, pil_loader, make_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beauty_train_raw = pd.read_csv('data/raw/beauty_data_info_train_competition.csv')\n",
    "df_fashion_train_raw = pd.read_csv('data/raw/fashion_data_info_train_competition.csv')\n",
    "df_mobile_train_raw = pd.read_csv('data/raw/mobile_data_info_train_competition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/beauty_profile_train.json', 'r') as file:\n",
    "    dict_beauty = json.load(file)\n",
    "with open('data/raw/fashion_profile_train.json', 'r') as file:\n",
    "    dict_fashion = json.load(file)\n",
    "with open('data/raw/mobile_profile_train.json', 'r') as file:\n",
    "    dict_mobile = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Operating System', 'Features', 'Network Connections', 'Memory RAM', 'Brand', 'Warranty Period', 'Storage Capacity', 'Color Family', 'Phone Model', 'Camera', 'Phone Screen Size'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mobile.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NA and convert to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_beauty = list(dict_beauty.keys())\n",
    "attributes_fashion = list(dict_fashion.keys())\n",
    "attributes_mobile = list(dict_mobile.keys())\n",
    "\n",
    "df_beauty_train_preprocessed = df_beauty_train_raw.copy()\n",
    "df_fashion_train_preprocessed = df_fashion_train_raw.copy()\n",
    "df_mobile_train_preprocessed = df_mobile_train_raw.copy()\n",
    "\n",
    "# df_beauty_train_preprocessed = df_beauty_train_preprocessed.dropna(how='all')\n",
    "# df_fashion_train_preprocessed = df_fashion_train_preprocessed.dropna(how='all')\n",
    "# df_mobile_train_preprocessed = df_mobile_train_preprocessed.dropna(how='all')\n",
    "\n",
    "\n",
    "df_beauty_train_preprocessed[attributes_beauty] = df_beauty_train_preprocessed[attributes_beauty].fillna(999).astype(int)\n",
    "df_fashion_train_preprocessed[attributes_fashion] = df_fashion_train_preprocessed[attributes_fashion].fillna(999).astype(int)\n",
    "df_mobile_train_preprocessed[attributes_mobile] = df_mobile_train_preprocessed[attributes_mobile].fillna(999).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding for attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty\n",
    "le_beauty_list, encoded_beauty_list = list(), list()\n",
    "for attribute in attributes_beauty:\n",
    "    le_beauty_attr = LabelEncoder()\n",
    "    encoded_beauty_attr = le_beauty_attr.fit_transform(df_beauty_train_preprocessed[attribute])\n",
    "    le_beauty_list.append(le_beauty_attr)\n",
    "    encoded_beauty_list.append(encoded_beauty_attr)\n",
    "    \n",
    "# Fashion\n",
    "le_fashion_list, encoded_fashion_list = list(), list()\n",
    "for attribute in attributes_fashion:\n",
    "    le_fashion_attr = LabelEncoder()\n",
    "    encoded_fashion_attr = le_fashion_attr.fit_transform(df_fashion_train_preprocessed[attribute])\n",
    "    le_fashion_list.append(le_fashion_attr)\n",
    "    encoded_fashion_list.append(encoded_fashion_attr)\n",
    "    \n",
    "# Mobile\n",
    "le_mobile_list, encoded_mobile_list = list(), list()\n",
    "for attribute in attributes_mobile:\n",
    "    le_mobile_attr = LabelEncoder()\n",
    "    encoded_mobile_attr = le_mobile_attr.fit_transform(df_mobile_train_preprocessed[attribute])\n",
    "    le_mobile_list.append(le_mobile_attr)\n",
    "    encoded_mobile_list.append(encoded_mobile_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num of classes for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty: [8, 300, 40, 10, 9]\n",
      "Fashion: [21, 17, 12, 20, 5]\n",
      "Mobile: [8, 8, 5, 11, 56, 15, 9, 22, 619, 16, 7]\n"
     ]
    }
   ],
   "source": [
    "num_classes_beauty = [len(le_beauty.classes_) for le_beauty in le_beauty_list]\n",
    "print('Beauty:', num_classes_beauty)\n",
    "\n",
    "num_classes_fashion = [len(le_fashion.classes_) for le_fashion in le_fashion_list]\n",
    "print('Fashion:', num_classes_fashion)\n",
    "\n",
    "num_classes_mobile = [len(le_mobile.classes_) for le_mobile in le_mobile_list]\n",
    "print('Mobile:', num_classes_mobile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beauty\n",
    "model_beauty_ft = models.resnet18(pretrained=True)\n",
    "# Freeze pre-trained layers\n",
    "for param in model_beauty_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "model_beauty_modified = ModifiedCNNv2(class_sizes=num_classes_beauty, attributes=attributes_beauty,\n",
    "                                      model_pretrained=model_beauty_ft)\n",
    "# print(model_beauty_modified)\n",
    "\n",
    "## Fashion\n",
    "model_fashion_ft = models.resnet18(pretrained=True)\n",
    "# Freeze pre-trained layers\n",
    "for param in model_fashion_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "model_fashion_modified = ModifiedCNNv2(class_sizes=num_classes_fashion, attributes=attributes_fashion,\n",
    "                                       model_pretrained=model_fashion_ft)\n",
    "# print(model_fashion_modified)\n",
    "\n",
    "## Mobile\n",
    "model_mobile_ft = models.resnet18(pretrained=True)\n",
    "# Freeze pre-trained layers\n",
    "for param in model_mobile_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "model_mobile_modified = ModifiedCNNv2(class_sizes=num_classes_mobile, attributes=attributes_mobile,\n",
    "                                      model_pretrained=model_mobile_ft)\n",
    "# print(model_fashion_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define device (CPU or CUDA), configure models for multiple GPUs and copy models to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedCNNv2(\n",
       "  (model_pretrained): DataParallel(\n",
       "    (module): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (Operating System): Linear(in_features=1000, out_features=8, bias=True)\n",
       "  (Features): Linear(in_features=1000, out_features=8, bias=True)\n",
       "  (Network Connections): Linear(in_features=1000, out_features=5, bias=True)\n",
       "  (Memory RAM): Linear(in_features=1000, out_features=11, bias=True)\n",
       "  (Brand): Linear(in_features=1000, out_features=56, bias=True)\n",
       "  (Warranty Period): Linear(in_features=1000, out_features=15, bias=True)\n",
       "  (Storage Capacity): Linear(in_features=1000, out_features=9, bias=True)\n",
       "  (Color Family): Linear(in_features=1000, out_features=22, bias=True)\n",
       "  (Phone Model): Linear(in_features=1000, out_features=619, bias=True)\n",
       "  (Camera): Linear(in_features=1000, out_features=16, bias=True)\n",
       "  (Phone Screen Size): Linear(in_features=1000, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_beauty_modified.set_multiple_gpus()\n",
    "model_beauty_modified.to(device)\n",
    "\n",
    "model_fashion_modified.set_multiple_gpus()\n",
    "model_fashion_modified.to(device)\n",
    "\n",
    "model_mobile_modified.set_multiple_gpus()\n",
    "model_mobile_modified.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_pipeline = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_beauty = ProductDataset(df=df_beauty_train_preprocessed, root_dir='data/raw',\n",
    "                                encoded_attributes=encoded_beauty_list, transform=transforms_pipeline)\n",
    "\n",
    "dataset_fashion = ProductDataset(df=df_fashion_train_preprocessed, root_dir='data/raw',\n",
    "                                encoded_attributes=encoded_fashion_list, transform=transforms_pipeline)\n",
    "\n",
    "dataset_mobile = ProductDataset(df=df_mobile_train_preprocessed, root_dir='data/raw',\n",
    "                                encoded_attributes=encoded_mobile_list, transform=transforms_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_beauty = optim.Adam(model_beauty_modified.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "optimizer_fashion = optim.Adam(model_fashion_modified.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "optimizer_mobile = optim.Adam(model_mobile_modified.parameters(), lr=1e-3, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8956 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxlee245/ndsc-2019/src/model_api_pytorch.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for c, attribute in zip(range(len(self.class_sizes)), self.attributes)}\n",
      "100%|██████████| 8956/8956 [20:38<00:00, 12.31it/s]\n",
      "  0%|          | 0/8956 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 13.8791\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8956/8956 [20:17<00:00,  7.35it/s]\n",
      "  0%|          | 0/8956 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 13.8712\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8956/8956 [20:21<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 13.8692\n",
      "\n",
      "Training completed in 61m 18s\n"
     ]
    }
   ],
   "source": [
    "model_beauty_trained = train_model(model=model_beauty_modified, dataset=dataset_beauty,\n",
    "                                   criterion=criterion, optimizer=optimizer_beauty, device=device, num_epochs=3,\n",
    "                                   batch_size=32, num_workers=12)\n",
    "torch.save(model_beauty_trained.state_dict(), 'models/weights-cnn-baseline-beauty-v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8599/8599 [32:25<00:00,  4.42it/s] \n",
      "  0%|          | 0/8599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 11.2597\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8599/8599 [29:13<00:00,  4.90it/s] \n",
      "  0%|          | 0/8599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 11.2378\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8599/8599 [26:32<00:00,  5.40it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 11.2379\n",
      "\n",
      "Training completed in 88m 11s\n"
     ]
    }
   ],
   "source": [
    "model_fashion_trained = train_model(model=model_fashion_modified, dataset=dataset_fashion,\n",
    "                                    criterion=criterion, optimizer=optimizer_fashion, device=device, num_epochs=3,\n",
    "                                    batch_size=32, num_workers=12)\n",
    "torch.save(model_fashion_trained.state_dict(), 'models/weights-cnn-baseline-fashion-v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxlee245/ndsc-2019/src/model_api_pytorch.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for c, attribute in zip(range(len(self.class_sizes)), self.attributes)}\n",
      "100%|██████████| 5011/5011 [13:39<00:00,  6.11it/s]\n",
      "  0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 25.6787\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5011/5011 [11:20<00:00,  7.36it/s]\n",
      "  0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 25.6566\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5011/5011 [11:22<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 25.6477\n",
      "\n",
      "Training completed in 36m 22s\n"
     ]
    }
   ],
   "source": [
    "model_mobile_trained = train_model(model=model_mobile_modified, dataset=dataset_mobile,\n",
    "                                   criterion=criterion, optimizer=optimizer_mobile, device=device, num_epochs=3,\n",
    "                                   batch_size=32, num_workers=12)\n",
    "torch.save(model_mobile_trained.state_dict(), 'models/weights-cnn-baseline-mobile-v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty\n",
    "model_beauty_saved = ModifiedCNNv2(class_sizes=num_classes_beauty, attributes=attributes_beauty,\n",
    "                                   model_pretrained=model_beauty_ft)\n",
    "model_beauty_saved.set_multiple_gpus()\n",
    "model_beauty_saved.to(device)\n",
    "model_beauty_saved.load_state_dict(torch.load('models/weights-cnn-baseline-beauty-v2.pt'))\n",
    "\n",
    "# Fashion\n",
    "model_fashion_saved = ModifiedCNNv2(class_sizes=num_classes_fashion, attributes=attributes_fashion,\n",
    "                                    model_pretrained=model_fashion_ft)\n",
    "model_fashion_saved.set_multiple_gpus()\n",
    "model_fashion_saved.to(device)\n",
    "model_fashion_saved.load_state_dict(torch.load('models/weights-cnn-baseline-fashion-v2.pt'))\n",
    "\n",
    "# Mobile\n",
    "model_mobile_saved = ModifiedCNNv2(class_sizes=num_classes_mobile, attributes=attributes_mobile,\n",
    "                                   model_pretrained=model_mobile_ft)\n",
    "model_mobile_saved.set_multiple_gpus()\n",
    "model_mobile_saved.to(device)\n",
    "model_mobile_saved.load_state_dict(torch.load('models/weights-cnn-baseline-mobile-v2.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transforms for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beauty_val = pd.read_csv('data/raw/beauty_data_info_val_competition.csv')\n",
    "df_fashion_val = pd.read_csv('data/raw/fashion_data_info_val_competition.csv')\n",
    "df_mobile_val = pd.read_csv('data/raw/mobile_data_info_val_competition.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframes for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76545 [00:00<?, ?it/s]/home/yxlee245/ndsc-2019/src/model_api_pytorch.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for c, attribute in zip(range(len(self.class_sizes)), self.attributes)}\n",
      "100%|██████████| 76545/76545 [2:00:14<00:00,  9.39it/s]  \n"
     ]
    }
   ],
   "source": [
    "df_beauty_submission = make_submission(df_val=df_beauty_val, model=model_beauty_saved, transforms=transforms_val,\n",
    "                                       root_dir='data/raw', le_list=le_beauty_list)\n",
    "df_beauty_submission.to_csv('data/derived/submission_cnn_baseline_beauty_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69498 [00:00<?, ?it/s]/home/yxlee245/ndsc-2019/src/model_api_pytorch.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for c, attribute in zip(range(len(self.class_sizes)), self.attributes)}\n",
      "100%|██████████| 69498/69498 [2:03:54<00:00,  8.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "df_fashion_submission = make_submission(df_val=df_fashion_val, model=model_fashion_saved, transforms=transforms_val,\n",
    "                                        root_dir='data/raw', le_list=le_fashion_list)\n",
    "df_fashion_submission.to_csv('data/derived/submission_cnn_baseline_fashion_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40417 [00:00<?, ?it/s]/home/yxlee245/ndsc-2019/src/model_api_pytorch.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for c, attribute in zip(range(len(self.class_sizes)), self.attributes)}\n",
      "100%|██████████| 40417/40417 [1:02:05<00:00,  7.78it/s]\n"
     ]
    }
   ],
   "source": [
    "df_mobile_submission = make_submission(df_val=df_mobile_val, model=model_mobile_saved, transforms=transforms_val,\n",
    "                                       root_dir='data/raw', le_list=le_mobile_list)\n",
    "df_mobile_submission.to_csv('data/derived/submission_cnn_baseline_mobile_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beauty_submission = pd.read_csv('data/derived/submission_cnn_baseline_beauty_v2.csv')\n",
    "df_fashion_submission = pd.read_csv('data/derived/submission_cnn_baseline_fashion_v2.csv')\n",
    "df_mobile_submission = pd.read_csv('data/derived/submission_cnn_baseline_mobile_v2.csv')\n",
    "df_submissions = pd.concat((df_beauty_submission, df_fashion_submission, df_mobile_submission), ignore_index=True)\n",
    "df_submissions.to_csv('data/submissions/submissions_cnn_baseline_20190312.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 0.11710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
