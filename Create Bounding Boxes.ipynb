{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of bounding boxes for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto update from code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.data import pil_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original CSVs\n",
    "df_beauty_train = pd.read_csv('data/raw/beauty_data_info_train_competition.csv')\n",
    "df_beauty_val = pd.read_csv('data/raw/beauty_data_info_val_competition.csv')\n",
    "\n",
    "df_fashion_train = pd.read_csv('data/raw/fashion_data_info_train_competition.csv')\n",
    "df_fashion_val = pd.read_csv('data/raw/fashion_data_info_val_competition.csv')\n",
    "\n",
    "df_mobile_train = pd.read_csv('data/raw/mobile_data_info_train_competition.csv')\n",
    "df_mobile_val = pd.read_csv('data/raw/mobile_data_info_val_competition.csv')\n",
    "    \n",
    "# Load label map\n",
    "with open('data/derived/label_map_beauty.json', 'r') as file:\n",
    "    label_map_beauty = json.load(file)\n",
    "    \n",
    "with open('data/derived/label_map_fashion.json', 'r') as file:\n",
    "    label_map_fashion = json.load(file)\n",
    "    \n",
    "with open('data/derived/label_map_mobile.json', 'r') as file:\n",
    "    label_map_mobile = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse label maps\n",
    "rev_label_map_beauty = {v: k for k, v in label_map_beauty.items()}\n",
    "rev_label_map_fashion = {v: k for k, v in label_map_fashion.items()}\n",
    "rev_label_map_mobile = {v: k for k, v in label_map_mobile.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipe = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 55. Best loss so far is 3.173.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (base): VGGBase(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_convs): AuxiliaryConvolutions(\n",
       "    (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (pred_convs): PredictionConvolutions(\n",
       "    (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv4_3): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv7): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv8_2): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv9_2): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv10_2): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv11_2): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = 'models/BEST_checkpoint_beauty.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "model_beauty = checkpoint['model']\n",
    "model_beauty = model_beauty.to(device)\n",
    "model_beauty.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286583/286583 [9:46:54<00:00,  8.03it/s]   \n"
     ]
    }
   ],
   "source": [
    "df_bb_beauty_train = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_beauty_train)) as pbar:\n",
    "    for image_path in df_beauty_train['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_beauty(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_beauty.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                        max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_beauty[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_beauty_train) == 0:\n",
    "                df_bb_beauty_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_beauty_train = df_bb_beauty_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'beauty' label if it exists\n",
    "        if 'beauty' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'beauty']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'beauty']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_beauty_train) == 0:\n",
    "                df_bb_beauty_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_beauty_train = df_bb_beauty_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_beauty_train) == 0:\n",
    "            df_bb_beauty_train = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_beauty_train = df_bb_beauty_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_beauty_train.to_csv('data/derived/bb_beauty_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76545/76545 [2:33:35<00:00,  8.20it/s]  \n"
     ]
    }
   ],
   "source": [
    "df_bb_beauty_val = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_beauty_val)) as pbar:\n",
    "    for image_path in df_beauty_val['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_beauty(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_beauty.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                        max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_beauty[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_beauty_val) == 0:\n",
    "                df_bb_beauty_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_beauty_val = df_bb_beauty_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'beauty' label if it exists\n",
    "        if 'beauty' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'beauty']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'beauty']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_beauty_val) == 0:\n",
    "                df_bb_beauty_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_beauty_val = df_bb_beauty_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_beauty_val) == 0:\n",
    "            df_bb_beauty_val = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_beauty_val = df_bb_beauty_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_beauty_val.to_csv('data/derived/bb_beauty_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 95. Best loss so far is 1.527.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (base): VGGBase(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_convs): AuxiliaryConvolutions(\n",
       "    (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (pred_convs): PredictionConvolutions(\n",
       "    (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv4_3): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv7): Conv2d(1024, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv8_2): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv9_2): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv10_2): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv11_2): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = 'models/BEST_checkpoint_fashion.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "model_fashion = checkpoint['model']\n",
    "model_fashion = model_fashion.to(device)\n",
    "model_fashion.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 201342/275142 [7:54:47<2:32:51,  8.05it/s] "
     ]
    }
   ],
   "source": [
    "df_bb_fashion_train = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_fashion_train)) as pbar:\n",
    "    for image_path in df_fashion_train['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_fashion(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_fashion.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                         max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_fashion[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_fashion_train) == 0:\n",
    "                df_bb_fashion_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_fashion_train = df_bb_fashion_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'fashion' label if it exists\n",
    "        if 'fashion' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'fashion']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'fashion']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_fashion_train) == 0:\n",
    "                df_bb_fashion_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_fashion_train = df_bb_fashion_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_fashion_train) == 0:\n",
    "            df_bb_fashion_train = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_fashion_train = df_bb_fashion_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_fashion_train.to_csv('data/derived/bb_fashion_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb_fashion_val = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_fashion_val)) as pbar:\n",
    "    for image_path in df_fashion_val['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_fashion(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_fashion.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                         max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_fashion[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_fashion_val) == 0:\n",
    "                df_bb_fashion_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_fashion_val = df_bb_fashion_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'fashion' label if it exists\n",
    "        if 'fashion' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'fashion']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'fashion']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_fashion_val) == 0:\n",
    "                df_bb_fashion_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_fashion_val = df_bb_fashion_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_fashion_val) == 0:\n",
    "            df_bb_fashion_val = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_fashion_val = df_bb_fashion_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_fashion_val.to_csv('data/derived/bb_fashion_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = 'models/BEST_checkpoint_mobile.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "model_mobile = checkpoint['model']\n",
    "model_mobile = model_mobile.to(device)\n",
    "model_mobile.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb_mobile_train = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_mobile_train)) as pbar:\n",
    "    for image_path in df_mobile_train['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_mobile(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_mobile.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                        max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_mobile[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_mobile_train) == 0:\n",
    "                df_bb_mobile_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_mobile_train = df_bb_mobile_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'mobile' label if it exists\n",
    "        if 'mobile' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'mobile']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'mobile']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_mobile_train) == 0:\n",
    "                df_bb_mobile_train = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_mobile_train = df_bb_mobile_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_mobile_train) == 0:\n",
    "            df_bb_mobile_train = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_mobile_train = df_bb_mobile_train.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_mobile_train.to_csv('data/derived/bb_mobile_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb_mobile_val = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(df_mobile_val)) as pbar:\n",
    "    for image_path in df_mobile_val['image_path']:\n",
    "        image_path_full = os.path.join('data/raw', image_path)\n",
    "        original_image = pil_loader(image_path_full)\n",
    "        input_tensor = transform_pipe(original_image).unsqueeze(0).to(device)\n",
    "        predicted_locs, predicted_scores = model_mobile(input_tensor)\n",
    "        det_boxes, det_labels, det_scores = model_mobile.detect_objects(predicted_locs, predicted_scores, min_score=0.8,\n",
    "                                                                        max_overlap=0.2, top_k=200)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map_mobile[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "        det_labels_np = np.array(det_labels)\n",
    "\n",
    "        # If no objects found, the bounding box coordinates would be the extremities of the original image\n",
    "        if det_labels == ['background']:\n",
    "            row_dict = {'image_path': image_path, 'x0': 0, 'y0':0,\n",
    "                        'x1': original_image.size[0],'y1':original_image.size[1]}\n",
    "            if len(df_bb_mobile_val) == 0:\n",
    "                df_bb_mobile_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_mobile_val = df_bb_mobile_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Move detections to the CPU\n",
    "        det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "        # Transform to original image dimensions and to NumPy array\n",
    "        original_dims = torch.FloatTensor(\n",
    "            [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "        det_boxes = det_boxes * original_dims\n",
    "        det_boxes_np = det_boxes.detach().numpy().round().astype(int) + 1\n",
    "\n",
    "        # Transform scores to NumPy array\n",
    "        det_scores_np = det_scores[0].to('cpu').detach().numpy()\n",
    "\n",
    "        # Prioritize 'mobile' label if it exists\n",
    "        if 'mobile' in det_labels:\n",
    "            det_scores_filtered = det_scores_np[det_labels_np == 'mobile']\n",
    "            det_boxes_filtered = det_boxes_np[det_labels_np == 'mobile']\n",
    "\n",
    "            det_boxes_best = det_boxes_filtered[det_scores_filtered.argmax()].flatten()\n",
    "\n",
    "            row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "            if len(df_bb_mobile_val) == 0:\n",
    "                df_bb_mobile_val = pd.DataFrame(row_dict,index=[0])\n",
    "            else:\n",
    "                df_bb_mobile_val = df_bb_mobile_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        # Record score\n",
    "        det_boxes_best = det_boxes_np[det_scores_np.argmax()].flatten()\n",
    "        row_dict = {'image_path': image_path, 'x0': det_boxes_best[0], 'y0':det_boxes_best[1],\n",
    "                        'x1': det_boxes_best[2],'y1':det_boxes_best[3]}\n",
    "        if len(df_bb_mobile_val) == 0:\n",
    "            df_bb_mobile_val = pd.DataFrame(row_dict,index=[0])\n",
    "        else:\n",
    "            df_bb_mobile_val = df_bb_mobile_val.append(row_dict, ignore_index=True)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_bb_mobile_val.to_csv('data/derived/bb_mobile_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
